{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15JbrrO4ih3JF7FuIXZaupHFN_dH_gx3a",
      "authorship_tag": "ABX9TyOccnfgvAWDeLUQcMihHnS9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orionhunts-ai/research/blob/main/Cyber_2_SQL_phi_small_8k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title HF Git { run: \"auto\" }\n",
        "#!git init\n",
        "!git config --global user.email \"core@synavate.tech\"\n",
        "!git config --global user.name \"Snyata\"\n",
        "!git remote set-url origin https://wwww.huggingface.co/synavate/cyber-tuned-phi3-8k.git\n",
        "!git add ./**\n",
        "!git commit -m \"Checkpoint 1 - Datset Cleaned\"\n",
        "!git push -u origin master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v37poPtG1dwT",
        "outputId": "1c8e81df-d542-4474-ccb4-befb1476ec25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: open(\"drive/MyDrive/AWS CPP/AWS Services in a Single Line.gdoc\"): Operation not supported\n",
            "error: unable to index file 'drive/MyDrive/AWS CPP/AWS Services in a Single Line.gdoc'\n",
            "fatal: adding files failed\n",
            "On branch master\n",
            "\n",
            "Initial commit\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31mdrive/\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "error: src refspec master does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://wwww.huggingface.co/synavate/cyber-tuned-phi3-8k.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install Core Libraries { run: \"auto\", display-mode: \"form\" }\n",
        "RUN=0\n",
        "if RUN == 0:\n",
        "  %pip -qqq install transformers torch datasets wandb\n",
        "  %pip -qqq install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "  %pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n",
        "\n",
        "  RUN += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQBEi9hbiuZU",
        "outputId": "18702ec8-26b2-40b6-d50c-95fc8b453851"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.1/302.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xformers<0.0.26\n",
            "  Downloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl\n",
            "  Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, xformers, trl, peft, accelerate\n",
            "Successfully installed accelerate-0.32.1 bitsandbytes-0.43.1 peft-0.11.1 trl-0.9.6 xformers-0.0.25.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phine Tuning Microsoft Research Phi-3-small-8k-instruct For Agentic Cyber 👾\n",
        "\n",
        "  *  Training (and reporting) on Google Colab\n",
        "   for access to their high powered CUDA and  \n",
        "  Leveraging [unsloth](https://github.com/unslothai) for efficient fine tuning, and conservative memory utilisation.\n",
        "    * Dataset for training is using SQL statements that were generated from Synthetic examples text thanks to Gretel.AI (100k examples to be exact!)\n",
        "    * Sampling from the full set for those that are most relevant to Cyber Security Analysts.***\n",
        "\n",
        "    * Aside from traditional and evolving Evaluations I will also deploy a number of the finely tuned models in a Microsoft Autogen agentic environment to see how they perform on basic analysis on a database.\n",
        "\n",
        "    * Red Panda (a high performance streaming data alternative to Kafka will be used)\n",
        "\n"
      ],
      "metadata": {
        "id": "SQrXn-C5brQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data and Tool Preparation\n",
        "**Summary:**\n",
        "\n",
        "This study explores the fine-tuning of the Phi-3-small-instruct model (7.39 billion parameters) by using Daniel So's Unsloth for a Cyber Threat Intelligence (CTI) task using methods like Parameter-Efficient Fine-Tuning (PEFT), Low-Rank Adaptation (LoRA), and Quantized Low-Rank Adaptation (QLoRA). It aims to evaluate performance degradation, model collaboration in agentic environments, and the potential influence of GPT-4. Synthetic data from gretel.ai was also utilized to supplement the fine-tuning process and enhance data diversity and robustness."
      ],
      "metadata": {
        "id": "Auy20cWWddxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -qqq huggingface_hub\n",
        "wandb.login(key=userdata.get('WANDB_API_KEY'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "0bAge5bNlyMg",
        "outputId": "cffde949-4c5e-43cf-e65f-7df8d2d2487f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wandb' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-95cae9ebb6e6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python -m pip install -qqq huggingface_hub'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'WANDB_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ML Ops and EDA Imports\n",
        "import os\n",
        "import wandb\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "\n",
        "wandb.init(project=\"Cyber-Phi-Small-8k\", )\n",
        "!huggingface-cli login --token $HF_TOKEN\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "xWDiksKlnZYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading GretelAI's Synthetic Dataset - Create your own at https://gretel.ai/'''\n",
        "\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
        "df = ds['train'].to_pandas()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cdfus0y8isnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Using dataframe df: make a dataframe called df_domain with only rows in the df_domains list and perform other data cleaning on the df_domain dataframe\n",
        "\n",
        "# Assuming df_domains is a list of domains to filter by\n",
        "df_domains = ['artificial intelligence','defense industry', 'defense operations', 'defense security', 'cybersecurity',\n",
        "              'defence contractors', 'technology']\n",
        "df_domain = df[df['domain'].isin(df_domains)].copy()\n",
        "\n",
        "\n",
        "df_domain = df_domain.drop_duplicates()\n",
        "df_domain = df_domain.dropna()\n",
        "df_domain.head()\n"
      ],
      "metadata": {
        "id": "SZrdTaP6uDvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: a vertical bar chart showing the distribution of domains in the dataframe\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'domain' is a column in your dataframe\n",
        "domain_counts = df_domain['domain'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "domain_counts.plot(kind='bar')\n",
        "plt.xlabel('Domains')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Domains')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_UnzP412uK3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_domain.shape)\n",
        "print(df_domain.columns)\n",
        "columns_to_drop=['sql_complexity', 'sql_complexity_description', 'sql_explanation', 'sql_context']\n",
        "df_clean = df_domain.drop(columns=columns_to_drop, inplace=False)"
      ],
      "metadata": {
        "id": "-5wPZwf0wMhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Data set is relatively balanced therefore there is no need for any manipulation or rebalancing of the domains. Fortunately, cybersecurity is the most well represented - Dataset Ready to go ***"
      ],
      "metadata": {
        "id": "lX8fJQPjv3Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(project=\"Cyber-Phi-Small-8k\", job_type=\"data-prep\")\n",
        "artifact = wandb.Artifact(name=\"data_set_cleaned\", type=\"data\")\n",
        "artifact.add_file()\n",
        "run.log_artifact(artifact)\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "HeZ3bG6pxKVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eEoFsHZTwIcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly from HuggingFace\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-small-8k-instruct\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "3e6F9rY2hyqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3MNsHmHxiAi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B6D5oLNFhyXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FolNEaAthdPq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvAi_yUZZrOW"
      },
      "outputs": [],
      "source": [
        "def load_and_log():\n",
        "\n",
        "    # 🚀 start a run, with a type to label it and a project it can call home\n",
        "    with wandb.init(project=\"artifacts-data-models\", job_type=\"load-data\") as run:\n",
        "\n",
        "        datasets = load()  # separate code for loading the datasets\n",
        "        names = [\"training\", \"validation\", \"test\"]\n",
        "\n",
        "        # 🏺 create our Artifact\n",
        "        raw_data = wandb.Artifact(\n",
        "            \"cyber-phi\", type=\"dataset\",\n",
        "            description=\"Cyber-Phi\",\n",
        "            metadata={\"source\": \"torchvision.datasets.MNIST\",\n",
        "                      \"sizes\": [len(dataset) for dataset in datasets]})\n",
        "\n",
        "        for name, data in zip(names, datasets):\n",
        "            # 🐣 Store a new file in the artifact, and write something into its contents.\n",
        "            with raw_data.new_file(name + \".pt\", mode=\"wb\") as file:\n",
        "                x, y = data.tensors\n",
        "                torch.save((x, y), file)\n",
        "\n",
        "        # ✍️ Save the artifact to W&B.\n",
        "        run.log_artifact(raw_data)\n",
        "\n",
        "load_and_log()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ijzGDrFhfRQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J8PHxUZ3fR-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "###APPENDIX A\n",
        "\n",
        "### 🤗Fine-Tuning Techniques: 🤗\n",
        "\n",
        "**PEFT** (Parameter-Efficient Fine-Tuning): Fine-tunes pre-trained models by adjusting only a small subset of parameters, reducing computational costs.\n",
        "\n",
        "**LoRA** (Low-Rank Adaptation): Enhances transformer models by injecting and training low-rank matrices within each layer, minimizing the number of trainable parameters.\n",
        "\n",
        "**QLoRA** (Quantized Low-Rank Adaptation): Combines low-rank adaptation with weight quantization to achieve efficient fine-tuning with reduced memory and computational requirements.\n",
        "\n",
        "**Full Fine-Tuning:** Updates all parameters of the pre-trained model, offering high flexibility at the cost of increased computational resources.\n",
        "\n",
        "**Distillation:** Trains a smaller model to mimic the behavior of a larger pre-trained model, optimizing efficiency while maintaining performance."
      ],
      "metadata": {
        "id": "nNiHj2N-daW4"
      }
    }
  ]
}